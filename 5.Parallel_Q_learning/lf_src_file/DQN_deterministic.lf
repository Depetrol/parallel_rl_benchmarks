target Python {
  threading: True
}

preamble {=
  #Import packages
  import time
  import gym
  import torch
  import torch.nn as nn
  import torch.optim as optim
  import numpy as np
  from collections import deque
  from random import sample
  import random

  # Define the Q-Network
  class QNetwork(nn.Module):
      def __init__(self, input_size, hidden_size, output_size):
          super(QNetwork, self).__init__()
          self.fc1 = nn.Linear(input_size, hidden_size)
          self.fc2 = nn.Linear(hidden_size, output_size)
          self.relu = nn.ReLU()

      def forward(self, state):
          x = self.relu(self.fc1(state))
          x = self.fc2(x)
          return x


  # Q-Network and Optimizer (in main process)
  device = "cuda" if torch.cuda.is_available() else "cpu"
  torch.manual_seed(1)
  q_net = QNetwork(3, 32, 2).to(device)
  target_net = QNetwork(3, 32, 2).to(device)
  target_net.load_state_dict(q_net.state_dict())
  target_net.eval()
  optimizer = optim.Adam(q_net.parameters(), lr=0.01)
  epsilon=0.1


  def update_q_network(batch, gamma=0.99):
      torch.manual_seed(1)
      torch.cuda.manual_seed_all(1)
      states, actions, rewards, next_states, dones = zip(*batch)
      states = torch.cat(states).to(device)
      next_states = torch.cat(next_states).to(device)
      actions = torch.tensor(actions).to(device)
      rewards = torch.tensor(rewards, dtype=torch.float32).to(device)
      dones = torch.tensor(dones, dtype=torch.float32).to(device)

      current_q_values = q_net(states).gather(
          1, actions.unsqueeze(-1)).squeeze(-1)
      next_q_values = target_net(next_states).max(1)[0]
      expected_q_values = rewards + gamma * next_q_values * (1 - dones)

      loss = nn.MSELoss()(current_q_values, expected_q_values.detach())

      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

  def log_dict_to_file(all_experiences):
      import datetime
      import os
      # Getting the current time to use in the filename
      current_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

      # File path for logging in the current directory
      file_path = f"all_experiences_{current_time}.txt"

      # Writing the experiences to a file
      with open(file_path, "w") as file:
          for key, value in all_experiences.items():
              file.write(f"{key}: {value}\n")

      return os.path.abspath(file_path)
=}

reactor RolloutReactor {
  input gradients
  output trajectories

  state env
  state obs
  state q_net
  state rng

  reaction(startup) {=
    random.seed(1)
    self.env = gym.make("Blackjack-v1")
    self.env.seed(1)
    self.obs = self.env.reset()
    self.rng = np.random.RandomState(1)
    self.q_net = QNetwork(3, 32, 2).to("cpu")
  =}

  reaction(gradients) -> trajectories {=
    random.seed(1)
    self.q_net.load_state_dict(gradients.value)
    experiences = []

    for _ in range(100):
        state_tensor = torch.from_numpy(
            np.array(self.obs)).float().unsqueeze(0)
        if self.rng.uniform(0, 1) < epsilon:
            action = self.rng.randint(0,2)
        else:
            with torch.no_grad():
                action = self.q_net(state_tensor).max(1)[1].item()

        next_state, reward, done, _ = self.env.step(action)
        next_state_tensor = torch.from_numpy(
            np.array(next_state)).float().unsqueeze(0)  # Convert to tensor
        # Use tensor here
        experiences.append(
            (state_tensor, action, reward, next_state_tensor, done))

        if done:
            self.obs = self.env.reset()
        else:
            self.obs = next_state

    trajectories.set(experiences)
  =}
}

reactor ReplayBufferReactor {
  input[16] trajectories
  output dataset

  state experiences

  reaction(startup) {=
    #Initialize ReplayBuffer
    self.experiences = deque(maxlen=20000)
  =}

  reaction(trajectories) -> dataset {=
    #Append Trajectories into ReplayBuffer
    for i in range(16):
      new_experiences = trajectories[i].value
      if new_experiences is not None:
          self.experiences.extend(new_experiences)

    result = list(self.experiences)[:min(250, len(self.experiences))]
    dataset.set(result)
  =}
}

reactor LearnerReactor {
  output gradients
  input dataset

  state start_time
  state q_net_state
  state round

  reaction(startup) -> gradients {=
    # Initialize the policy
    self.start_time = time.time()
    self.round = 0
    self.q_net_state = {key: tensor.to("cpu") for key, tensor in q_net.state_dict().items()}
    gradients.set(self.q_net_state)
  =}

  reaction(dataset) -> gradients {=
    random.seed(1)
    # Update the policy
    batch = dataset.value
    if batch:
        update_q_network(batch)

    self.q_net_state = {key: tensor.to("cpu") for key, tensor in q_net.state_dict().items()}
    print(self.q_net_state["fc1.weight"][5])
    if self.round % 10 == 0:  # Update target network every 10 rounds
        target_net.load_state_dict(q_net.state_dict())

    print(f"Round {self.round}, Training Time: {time.time()-self.start_time:.2f}")
    self.round+=1


    log_dict_to_file(self.q_net_state)

    gradients.set(self.q_net_state)
  =}
}

main reactor {
  rollout = new[16] RolloutReactor()
  replay = new ReplayBufferReactor()
  learner = new LearnerReactor()

  (learner.gradients)+ -> rollout.gradients after 0
  rollout.trajectories -> replay.trajectories
  replay.dataset -> learner.dataset
}
